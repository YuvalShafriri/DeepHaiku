# **Deep Haiku: Teaching GPT-J to Compose with Syllable Patterns**
## How to generate rhythmic prose after fine-tuning a large transformer with phonemes

By Robert. A Gonsalves</br>

![image](https://raw.githubusercontent.com/robgon-art/DeepHaiku/main/deep_haiku.jpg)

You can see my article on [Medium](https://towardsdatascience.com/deep-haiku-teaching-gpt-j-to-compose-with-syllable-patterns-5234bca9701).

The source code and generated Haikus are released under the [CC BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/).</br>
![CC BYC-SA](https://licensebuttons.net/l/by-sa/3.0/88x31.png)

## Google Colabs
* [Deep Haiku Trainer](https://colab.research.google.com/github/robgon-art/DeepHaiku/blob/main/Deep_Haiku_Train.ipynb)
* [Deep Haiku Generator](https://colab.research.google.com/github/robgon-art/DeepHaiku/blob/main/Deep_Haiku_Generator.ipynb)

## Acknowledgements
- M. Grootendorst, KeyBERT: Minimal keyword extraction with BERT (2020)
- W. Zhu and S. Bhat, GRUEN for Evaluating Linguistic Quality of Generated Text (2020)
- M. Bernard, Phonemizer: Text to Phones Transcription for Multiple Languages in Python (2016)
- GPT-J, Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX (2021)
- R. Caruana, Multitask learning (1997)
- E. Hu, et al., LoRA: Low-rank Adaptation of Large Language Models (2021)
- L. Hanu and the Unitary Team, Detoxify (2020)
- Trained on Haikus collected by [bfbarry](https://www.kaggle.com/bfbarry/haiku-dataset) and [Harshit Jhalani](https://www.kaggle.com/hjhalani30/haiku-dataset) on Kaggle.com

## Citation
To cite this repository:

```bibtex
@software{DeepHaiku,
  author  = {Gonsalves, Robert A.},
  title   = {Deep Haiku: Teaching GPT-J to Compose with Syllable Patterns},
  url     = {https://github.com/robgon-art/DeepHaiku},
  year    = 2022,
  month   = February
}
```
