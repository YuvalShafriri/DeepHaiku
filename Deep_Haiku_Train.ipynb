{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robgon-art/DeepHaiku/blob/main/Deep_Haiku_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLzX_EPqAnEW"
      },
      "source": [
        "# **Deep Haiku Trainer**\n",
        "## Generating rhythmic prose after finetuning a large transformer withÂ phonemes\n",
        "\n",
        "By Robert. A Gonsalves</br>\n",
        "\n",
        "![image](https://raw.githubusercontent.com/robgon-art/DeepHaiku/main/deep_haiku.jpg)\n",
        "\n",
        "You can see my article on Medium.\n",
        "\n",
        "The source code and generated images are released under the [CC BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/).</br>\n",
        "![CC BYC-SA](https://licensebuttons.net/l/by-sa/3.0/88x31.png)\n",
        "\n",
        "## Acknowledgements\n",
        "- GPT-J, Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX (2021)\n",
        "- R. Caruana, Multitask learning (1997)\n",
        "- E. Hu, et al., LoRA: Low-rank Adaptation of Large Language Models (2021)\n",
        "- Trained on Haikus collected by [bfbarry](https://www.kaggle.com/bfbarry/haiku-dataset) and [Harshit Jhalani](https://www.kaggle.com/hjhalani30/haiku-dataset) on Kaggle.com\n",
        "\n",
        "This notebook is a proof of concept for fine-tuning [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B) with limited memory. A detailed explanation of how it works can be found in [this model card](https://huggingface.co/hivemind/gpt-j-6B-8bit).\n",
        "\n",
        "Adapted from here: https://colab.research.google.com/drive/1ft6wQU0BhqG5PRlwgaZJv2VukKKjU4Es\n",
        "\n",
        "And Nikita Schneider's article, [here](https://medium.com/geekculture/fine-tune-eleutherai-gpt-neo-to-generate-netflix-movie-descriptions-in-only-47-lines-of-code-40c9b4c32475)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "yHW45ab76mIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "haikus_g = []\n",
        "haikus_p = []\n",
        "topics_g = []\n",
        "topics_p = []\n",
        "count = 0\n",
        "\n",
        "!gdown --id 1ZKJiMRFwzkuJwjzhpadzAHt3KxhKNKiV\n",
        "\n",
        "with open(\"Deep_Haiku.csv\", newline='') as csv_read_file:\n",
        "  reader = csv.DictReader(csv_read_file)\n",
        "\n",
        "  for row in reader:\n",
        "    haikus_g.append(row[\"Haiku G\"])\n",
        "    haikus_p.append(row[\"Haiku P\"])\n",
        "    topics_g.append(row[\"Topic G\"])\n",
        "    topics_p.append(row[\"Topic P\"])\n",
        "    count +=1\n",
        "\n",
        "    # if count >= 5:\n",
        "    #   break\n",
        "\n",
        "print(count)"
      ],
      "metadata": {
        "id": "PxaeJEp4tUTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# graphemes (topic_g = haiku_g)\n",
        "# phonemes  <topic_p = haik_p>\n",
        "# g2p       [haiku_g = haiku_p]\n",
        "# p2g       {haiku_p = haiku_g}\n",
        "\n",
        "# (encouragement = Need encouragement. / Making myself positive. / I want happiness.)\n",
        "# <axn|ker|axjh|maxnt = niyd axn|ker|axjh|maxnt / mey|kaxng may|sehlf paa|zax|tihv / ay waant hhae|piy|naxsy>\n",
        "# [need encouragement / making myself positive / i want happiness = niyd axn|ker|axjh|maxnt / mey|kaxng may|sehlf paa|zax|tihv / ay waant hhae|piy|naxs]  \n",
        "# {niyd axn|ker|axjh|maxnt / mey|kaxng may|sehlf paa|zax|tihv / ay waant hhae|piy|naxs = need encouragement / making myself positive / i want happiness}\n",
        "\n",
        "data = []\n",
        "\n",
        "# text generation from topics using graphines\n",
        "for t, h in zip(topics_g, haikus_g):\n",
        "  line = \"(\"+ t + \" = \" + h + \")\"\n",
        "  data.append(line)\n",
        "\n",
        "# text generation from topics using ponemes\n",
        "for t, h in zip(topics_p, haikus_p):\n",
        "  line = \"<\"+ t + \" = \" + h + \">\"\n",
        "  data.append(line)\n",
        "\n",
        "# translation from graphemes to phonemes and back\n",
        "for g, p in zip(haikus_g, haikus_p):\n",
        "  line = \"[\" + g + \" = \" + p + \"]\"\n",
        "  data.append(line)\n",
        "  line = \"{\" + p + \" = \" + g + \"}\"\n",
        "  data.append(line)\n",
        "\n",
        "random.shuffle(data)\n",
        "\n",
        "for d in data[:20]:\n",
        "  print(d)"
      ],
      "metadata": {
        "id": "y80V6t9Vxnli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op0GXmC8CCyR"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install bitsandbytes\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0dy1ZFwClcq"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.cuda.amp import custom_fwd, custom_bwd\n",
        "from bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GODiktIBFt4w"
      },
      "source": [
        "### Converting the model to 8 bits.\n",
        "\n",
        "We convert EleutherAI's GPT-J-6B model to 8 bits using facebook's [bitsandbytes](https://github.com/facebookresearch/bitsandbytes) library. This reduces the model's size from 20Gb down to just 6Gb.\n",
        "\n",
        "Note that we don't convert linear layer biases to 8 bit as they take up less that 1% of the model's weight anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8Y75B6WDIN-"
      },
      "outputs": [],
      "source": [
        "class FrozenBNBLinear(nn.Module):\n",
        "    def __init__(self, weight, absmax, code, bias=None):\n",
        "        assert isinstance(bias, nn.Parameter) or bias is None\n",
        "        super().__init__()\n",
        "        self.out_features, self.in_features = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        "        self.bias = bias\n",
        " \n",
        "    def forward(self, input):\n",
        "        output = torch.clone(DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias))\n",
        "        if self.adapter:\n",
        "            output += self.adapter(input)\n",
        "        return output\n",
        " \n",
        "    @classmethod\n",
        "    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n",
        "        return cls(weights_int8, *state, linear.bias)\n",
        " \n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n",
        " \n",
        " \n",
        "class DequantizeAndLinear(torch.autograd.Function): \n",
        "    @staticmethod\n",
        "    @custom_fwd\n",
        "    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n",
        "                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        ctx.save_for_backward(input, weights_quantized, absmax, code)\n",
        "        ctx._has_bias = bias is not None\n",
        "        return F.linear(input, weights_deq, bias)\n",
        " \n",
        "    @staticmethod\n",
        "    @custom_bwd\n",
        "    def backward(ctx, grad_output: torch.Tensor):\n",
        "        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n",
        "        input, weights_quantized, absmax, code = ctx.saved_tensors\n",
        "        # grad_output: [*batch, out_features]\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        grad_input = grad_output @ weights_deq\n",
        "        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n",
        "        return grad_input, None, None, None, grad_bias\n",
        " \n",
        " \n",
        "class FrozenBNBEmbedding(nn.Module):\n",
        "    def __init__(self, weight, absmax, code):\n",
        "        super().__init__()\n",
        "        self.num_embeddings, self.embedding_dim = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        " \n",
        "    def forward(self, input, **kwargs):\n",
        "        with torch.no_grad():\n",
        "            # note: both quantuized weights and input indices are *not* differentiable\n",
        "            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n",
        "            output = F.embedding(input, weight_deq, **kwargs)\n",
        "        if self.adapter:\n",
        "            output += self.adapter(input)\n",
        "        return output \n",
        " \n",
        "    @classmethod\n",
        "    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n",
        "        return cls(weights_int8, *state)\n",
        " \n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n",
        " \n",
        " \n",
        "def quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2 ** 20):\n",
        "    assert chunk_size % 4096 == 0\n",
        "    code = None\n",
        "    chunks = []\n",
        "    absmaxes = []\n",
        "    flat_tensor = matrix.view(-1)\n",
        "    for i in range((matrix.numel() - 1) // chunk_size + 1):\n",
        "        input_chunk = flat_tensor[i * chunk_size: (i + 1) * chunk_size].clone()\n",
        "        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n",
        "        chunks.append(quantized_chunk)\n",
        "        absmaxes.append(absmax_chunk)\n",
        " \n",
        "    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n",
        "    absmax = torch.cat(absmaxes)\n",
        "    return matrix_i8, (absmax, code)\n",
        " \n",
        " \n",
        "def convert_to_int8(model):\n",
        "    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n",
        "    for module in list(model.modules()):\n",
        "        for name, child in module.named_children():\n",
        "            if isinstance(child, nn.Linear):\n",
        "                print(name, child)\n",
        "                setattr( \n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBLinear(\n",
        "                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                        bias=child.bias,\n",
        "                    ),\n",
        "                )\n",
        "            elif isinstance(child, nn.Embedding):\n",
        "                setattr(\n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBEmbedding(\n",
        "                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                    )\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOSZ-S1cDRq1"
      },
      "outputs": [],
      "source": [
        "class GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        convert_to_int8(self.attn)\n",
        "        convert_to_int8(self.mlp)\n",
        "\n",
        "\n",
        "class GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "        \n",
        "\n",
        "class GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "\n",
        "\n",
        "transformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock  # monkey-patch GPT-J"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pthEhmDBSyEm"
      },
      "outputs": [],
      "source": [
        "config = transformers.GPTJConfig.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuW4H6HTS82r"
      },
      "outputs": [],
      "source": [
        "gpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\") #, low_cpu_mem_usage=True)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "gpt.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRF2QqvzENCr"
      },
      "source": [
        "### Text generation example"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  prompt_tokens2 = tokenizer(\"My pet pug is\", return_tensors=\"pt\").input_ids.cuda()\n",
        "  sample_outputs = gpt.generate(prompt_tokens2, max_length=40, do_sample=True, temperature=0.7)\n",
        "  for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "id": "hC4ebGUSL8Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfdLQHOuEU7h"
      },
      "source": [
        "### LoRA fine-tuning example\n",
        "Here we demonstrate how to fine-tune the proposed model using low-rank adapters [(Hu et al, 2021)](https://arxiv.org/abs/2106.09685) and [8-bit Adam](https://arxiv.org/abs/2110.02861). We also use [dataset streaming API](https://huggingface.co/docs/datasets/dataset_streaming.html) to avoid downloading the large dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5ctu4Q5aq-g"
      },
      "outputs": [],
      "source": [
        "def add_adapters(model, adapter_dim=16):\n",
        "    assert adapter_dim > 0\n",
        "\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, FrozenBNBLinear):\n",
        "            module.adapter = nn.Sequential(\n",
        "                nn.Linear(module.in_features, adapter_dim, bias=False),\n",
        "                nn.Linear(adapter_dim, module.out_features, bias=False),\n",
        "            )\n",
        "            nn.init.zeros_(module.adapter[1].weight)\n",
        "        elif isinstance(module, FrozenBNBEmbedding):\n",
        "            module.adapter = nn.Sequential(\n",
        "                nn.Embedding(module.num_embeddings, adapter_dim),\n",
        "                nn.Linear(adapter_dim, module.embedding_dim, bias=False),\n",
        "            )\n",
        "            nn.init.zeros_(module.adapter[1].weight)\n",
        "\n",
        "add_adapters(gpt)\n",
        "gpt.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, random_split\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "class HaikuDataset(Dataset):\n",
        "    def __init__(self, txt_list, tokenizer, max_length):\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.labels = []\n",
        "        packed_text = \"\"\n",
        "        for i, txt in enumerate(txt_list):\n",
        "            packed_text += txt\n",
        "            # print(i, packed_text)\n",
        "            \n",
        "            if i%8 == 7:\n",
        "                encodings_dict = tokenizer(packed_text, truncation=True,\n",
        "                                          max_length=max_length, padding=\"max_length\")\n",
        "                self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "                self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "                packed_text = \"\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]"
      ],
      "metadata": {
        "id": "jxlivhV4j5ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(tokenizer(d)[0]) for d in data])\n",
        "print(max_length)"
      ],
      "metadata": {
        "id": "HAra5nGSkCux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "dataset = HaikuDataset(data, tokenizer, max_length=max_length)"
      ],
      "metadata": {
        "id": "uDLfhho9kDEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir checkpointscheckpoints\n",
        "!mkdir /content/checkpoints/output\n",
        "!mkdir /content/checkpoints/logs"
      ],
      "metadata": {
        "id": "MdX0lP4pdvID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset))"
      ],
      "metadata": {
        "id": "6KUX6VqKmW8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class SaveCallback(TrainerCallback):\n",
        "  \"A callback that prints a message at the beginning of training\"\n",
        "\n",
        "  def on_step_end(self, args, state, control, **kwargs):\n",
        "    if state.global_step %5000 == 4999:\n",
        "      file_name = \"/content/checkpoints/output/gpt-j-8bit_full_\" + str(state.global_step+1).zfill(6) + \".pt\"\n",
        "      torch.save(gpt, file_name)"
      ],
      "metadata": {
        "id": "OM7hu0yO_Qdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.95 * len(dataset))\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
        "training_args = TrainingArguments(output_dir=\"/content/checkpoints/\",num_train_epochs=5, logging_steps=1000,\n",
        "                                  save_strategy=\"no\", per_device_train_batch_size=2, per_device_eval_batch_size=2,\n",
        "                                  warmup_steps=100, weight_decay=0.01, logging_dir=\"logs\")\n",
        "Trainer(model=gpt, args=training_args, train_dataset=train_dataset, callbacks=[SaveCallback],\n",
        "        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
        "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
        "                                                              'labels': torch.stack([f[0] for f in data])}).train()\n",
        "\n",
        "torch.save(gpt, \"/content/checkpoints/gpt-j-8bit_full.pt\")"
      ],
      "metadata": {
        "id": "ZBrdDCq9LIh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_tokens = tokenizer(\"(pet pug\", return_tensors=\"pt\").input_ids.cuda()\n",
        "sample_outputs = gpt.generate(prompt_tokens, max_length=85, do_sample=True, \n",
        "  num_return_sequences=5, temperature=0.8)"
      ],
      "metadata": {
        "id": "SxkhHx47LUPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  doc = (tokenizer.decode(sample_outputs[i], skip_special_tokens=True))\n",
        "  parts = doc.split(\")\")\n",
        "  print(parts[0][1:])"
      ],
      "metadata": {
        "id": "5y1nYRaPqkIv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep Haiku Train",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}